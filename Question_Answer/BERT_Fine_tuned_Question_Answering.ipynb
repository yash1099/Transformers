{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQk9G7nkN1EU"
      },
      "source": [
        "# Finetuning Question Answering on BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_YTYbFn0FRL"
      },
      "source": [
        "**Fine_tuning BERT Extracitve Question Answering in PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdmrH8Tan8wC",
        "outputId": "5a671fe2-6775-4e68-b566-bd22ee0e913a"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.memory.empty_cache() -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPoexZG8L9w8"
      },
      "source": [
        "Install transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD2QijQuMC2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2968ec7b-b0bd-4d0a-ec23-408a801cb9e5"
      },
      "source": [
        "!pip install -q transformers datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phCxmwEPMQlS"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbluAigMPqq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast, get_linear_schedule_with_warmup\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHIawrVPfG0"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeEbZWRVPl2B"
      },
      "source": [
        "from datasets import load_dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beUalV5_Pnqm"
      },
      "source": [
        "### Load and split dataset, using small datasets for the sake of model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwoGxKpGJVpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62cc6de-7ed4-4d37-9372-f70ec8c5140a"
      },
      "source": [
        "train_data, valid_data = load_dataset('squad_v2', split='train[:1%]'), load_dataset('squad_v2', split='validation[:3%]')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n",
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRvHqejN0Z4I",
        "outputId": "f40e4dc0-0a30-4338-b6b4-1b4db7fcdd00"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56be85543aeaaa14008c9063',\n",
              " 'title': 'Beyoncé',\n",
              " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'question': 'When did Beyonce start becoming popular?',\n",
              " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSiNoRZDkdv0"
      },
      "source": [
        "### Getting correct answer text alignment and tokenizing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Pmox87IuwU"
      },
      "source": [
        "# Dataset cleaning and tokenization\n",
        "# BertTokenizerFast because python tokenizer do not have char_to_token functionality\n",
        "\n",
        "def correct_alignment(context, answer):\n",
        "\n",
        "    \"\"\" Description: This functions corrects the alignment of answers in the squad dataset that are sometimes off by one or 2 values also adds end_postion index.\n",
        "    \n",
        "    inputs: list of contexts and answers\n",
        "    outputs: Updated list that contains answer_end positions \"\"\"\n",
        "    \n",
        "    start_text = answer['text'][0]\n",
        "    start_idx = answer['answer_start'][0]\n",
        "    end_idx = start_idx + len(start_text)\n",
        "\n",
        "    # When alignment is okay\n",
        "    if context[start_idx:end_idx] == start_text:\n",
        "      return start_idx, end_idx    \n",
        "      # When alignment is off by 1 character\n",
        "    elif context[start_idx-1:end_idx-1] == start_text:\n",
        "      return start_idx-1, end_idx-1  \n",
        "      # when alignment is off by 2 characters\n",
        "    elif context[start_idx-2:end_idx-2] == start_text:\n",
        "      return start_idx-2, end_idx-2\n",
        "    else:\n",
        "      raise ValueError()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhaGUEvdP_TA"
      },
      "source": [
        "### Tokenize our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "metadata": {
        "id": "XoThaBRk-QSg"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_num = np.random.randint(0,len(train_data))\n",
        "question = train_data[\"question\"][random_num]\n",
        "text = train_data[\"context\"][random_num]"
      ],
      "metadata": {
        "id": "8GbyHAjUIvi0"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3jrAY1AIvpT",
        "outputId": "df47a588-3f8a-409c-8226-f406b6fe3009"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 315 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWGpklAlJ1N0",
        "outputId": "e0b85c14-7f10-424f-f0db-26520bce336e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]        101\n",
            "how        2,129\n",
            "many       2,116\n",
            "did        2,106\n",
            "crazy      4,689\n",
            "in         1,999\n",
            "love       2,293\n",
            "sell       5,271\n",
            "to         2,000\n",
            "become     2,468\n",
            "one        2,028\n",
            "of         1,997\n",
            "the        1,996\n",
            "greatest   4,602\n",
            "selling    4,855\n",
            "singles    3,895\n",
            "in         1,999\n",
            "history    2,381\n",
            "?          1,029\n",
            "[SEP]        102\n",
            "her        2,014\n",
            "debut      2,834\n",
            "single     2,309\n",
            ",          1,010\n",
            "\"          1,000\n",
            "crazy      4,689\n",
            "in         1,999\n",
            "love       2,293\n",
            "\"          1,000\n",
            "was        2,001\n",
            "named      2,315\n",
            "vh1       26,365\n",
            "'          1,005\n",
            "s          1,055\n",
            "\"          1,000\n",
            "greatest   4,602\n",
            "song       2,299\n",
            "of         1,997\n",
            "the        1,996\n",
            "2000s      8,876\n",
            "\"          1,000\n",
            ",          1,010\n",
            "nme       23,770\n",
            "'          1,005\n",
            "s          1,055\n",
            "\"          1,000\n",
            "best       2,190\n",
            "track      2,650\n",
            "of         1,997\n",
            "the        1,996\n",
            "00         4,002\n",
            "##s        2,015\n",
            "\"          1,000\n",
            "and        1,998\n",
            "\"          1,000\n",
            "pop        3,769\n",
            "song       2,299\n",
            "of         1,997\n",
            "the        1,996\n",
            "century    2,301\n",
            "\"          1,000\n",
            ",          1,010\n",
            "considered   2,641\n",
            "by         2,011\n",
            "rolling    5,291\n",
            "stone      2,962\n",
            "to         2,000\n",
            "be         2,022\n",
            "one        2,028\n",
            "of         1,997\n",
            "the        1,996\n",
            "500        3,156\n",
            "greatest   4,602\n",
            "songs      2,774\n",
            "of         1,997\n",
            "all        2,035\n",
            "time       2,051\n",
            ",          1,010\n",
            "earned     3,687\n",
            "two        2,048\n",
            "grammy     8,922\n",
            "awards     2,982\n",
            "and        1,998\n",
            "is         2,003\n",
            "one        2,028\n",
            "of         1,997\n",
            "the        1,996\n",
            "best       2,190\n",
            "-          1,011\n",
            "selling    4,855\n",
            "singles    3,895\n",
            "of         1,997\n",
            "all        2,035\n",
            "time       2,051\n",
            "at         2,012\n",
            "around     2,105\n",
            "8          1,022\n",
            "million    2,454\n",
            "copies     4,809\n",
            ".          1,012\n",
            "the        1,996\n",
            "music      2,189\n",
            "video      2,678\n",
            "for        2,005\n",
            "\"          1,000\n",
            "single     2,309\n",
            "ladies     6,456\n",
            "(          1,006\n",
            "put        2,404\n",
            "a          1,037\n",
            "ring       3,614\n",
            "on         2,006\n",
            "it         2,009\n",
            ")          1,007\n",
            "\"          1,000\n",
            ",          1,010\n",
            "which      2,029\n",
            "achieved   4,719\n",
            "fame       4,476\n",
            "for        2,005\n",
            "its        2,049\n",
            "intricate  17,796\n",
            "choreography  16,967\n",
            "and        1,998\n",
            "its        2,049\n",
            "deployment  10,813\n",
            "of         1,997\n",
            "jazz       4,166\n",
            "hands      2,398\n",
            ",          1,010\n",
            "was        2,001\n",
            "credited   5,827\n",
            "by         2,011\n",
            "the        1,996\n",
            "toronto    4,361\n",
            "star       2,732\n",
            "as         2,004\n",
            "having     2,383\n",
            "started    2,318\n",
            "the        1,996\n",
            "\"          1,000\n",
            "first      2,034\n",
            "major      2,350\n",
            "dance      3,153\n",
            "cr        13,675\n",
            "##az      10,936\n",
            "##e        2,063\n",
            "of         1,997\n",
            "both       2,119\n",
            "the        1,996\n",
            "new        2,047\n",
            "millennium  10,144\n",
            "and        1,998\n",
            "the        1,996\n",
            "internet   4,274\n",
            "\"          1,000\n",
            ",          1,010\n",
            "triggering  29,170\n",
            "a          1,037\n",
            "number     2,193\n",
            "of         1,997\n",
            "par       11,968\n",
            "##odies   27,391\n",
            "of         1,997\n",
            "the        1,996\n",
            "dance      3,153\n",
            "choreography  16,967\n",
            "and        1,998\n",
            "a          1,037\n",
            "legion     8,009\n",
            "of         1,997\n",
            "amateur    5,515\n",
            "im        10,047\n",
            "##ita      6,590\n",
            "##tors     6,591\n",
            "on         2,006\n",
            "youtube    7,858\n",
            ".          1,012\n",
            "in         1,999\n",
            "2013       2,286\n",
            ",          1,010\n",
            "drake      7,867\n",
            "released   2,207\n",
            "a          1,037\n",
            "single     2,309\n",
            "titled     4,159\n",
            "\"          1,000\n",
            "girls      3,057\n",
            "love       2,293\n",
            "beyonce   20,773\n",
            "\"          1,000\n",
            ",          1,010\n",
            "which      2,029\n",
            "featured   2,956\n",
            "an         2,019\n",
            "inter      6,970\n",
            "##pol     18,155\n",
            "##ation    3,370\n",
            "from       2,013\n",
            "destiny   10,461\n",
            "child      2,775\n",
            "'          1,005\n",
            "s          1,055\n",
            "\"          1,000\n",
            "say        2,360\n",
            "my         2,026\n",
            "name       2,171\n",
            "\"          1,000\n",
            "and        1,998\n",
            "discussed   6,936\n",
            "his        2,010\n",
            "relationship   3,276\n",
            "with       2,007\n",
            "women      2,308\n",
            ".          1,012\n",
            "in         1,999\n",
            "january    2,254\n",
            "2012       2,262\n",
            ",          1,010\n",
            "research   2,470\n",
            "scientist   7,155\n",
            "bryan      8,527\n",
            "less       2,625\n",
            "##ard      4,232\n",
            "named      2,315\n",
            "sc         8,040\n",
            "##ap       9,331\n",
            "##tia     10,711\n",
            "beyonce   20,773\n",
            "##ae       6,679\n",
            ",          1,010\n",
            "a          1,037\n",
            "species    2,427\n",
            "of         1,997\n",
            "horse      3,586\n",
            "fly        4,875\n",
            "found      2,179\n",
            "in         1,999\n",
            "northern   2,642\n",
            "queensland   5,322\n",
            ",          1,010\n",
            "australia   2,660\n",
            "after      2,044\n",
            "beyonce   20,773\n",
            "due        2,349\n",
            "to         2,000\n",
            "the        1,996\n",
            "fly        4,875\n",
            "'          1,005\n",
            "s          1,055\n",
            "unique     4,310\n",
            "golden     3,585\n",
            "hairs     13,606\n",
            "on         2,006\n",
            "its        2,049\n",
            "abdomen   13,878\n",
            ".          1,012\n",
            "in         1,999\n",
            "july       2,251\n",
            "2014       2,297\n",
            ",          1,010\n",
            "a          1,037\n",
            "beyonce   20,773\n",
            "exhibit    8,327\n",
            "was        2,001\n",
            "introduced   3,107\n",
            "into       2,046\n",
            "the        1,996\n",
            "\"          1,000\n",
            "legends    9,489\n",
            "of         1,997\n",
            "rock       2,600\n",
            "\"          1,000\n",
            "section    2,930\n",
            "of         1,997\n",
            "the        1,996\n",
            "rock       2,600\n",
            "and        1,998\n",
            "roll       4,897\n",
            "hall       2,534\n",
            "of         1,997\n",
            "fame       4,476\n",
            ".          1,012\n",
            "the        1,996\n",
            "black      2,304\n",
            "leo        6,688\n",
            "##tar      7,559\n",
            "##d        2,094\n",
            "from       2,013\n",
            "the        1,996\n",
            "\"          1,000\n",
            "single     2,309\n",
            "ladies     6,456\n",
            "\"          1,000\n",
            "video      2,678\n",
            "and        1,998\n",
            "her        2,014\n",
            "outfit    11,018\n",
            "from       2,013\n",
            "the        1,996\n",
            "super      3,565\n",
            "bowl       4,605\n",
            "half       2,431\n",
            "time       2,051\n",
            "performance   2,836\n",
            "are        2,024\n",
            "among      2,426\n",
            "several    2,195\n",
            "pieces     4,109\n",
            "housed     7,431\n",
            "at         2,012\n",
            "the        1,996\n",
            "museum     2,688\n",
            ".          1,012\n",
            "[SEP]        102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)\n",
        "#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)\n",
        "#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWpQjdgYIvt5",
        "outputId": "3203ce66-d1d4-4fd1-e8d8-94530bd48b6b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEP token index:  19\n",
            "Number of tokens in segment A:  20\n",
            "Number of tokens in segment B:  295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
        "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
      ],
      "metadata": {
        "id": "84WVe8MmIvz5"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens with highest start and end scores\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "if answer_end >= answer_start:\n",
        "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "    \n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sChS31PLJlFN",
        "outputId": "4b588d61-4cc6-4519-a8b3-a244f52c26e3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            "How many did crazy in love sell to become one of the greatest selling singles in history?\n",
            "\n",
            "Answer:\n",
            "8 million.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "JmlPOmwwN6GG"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_rXKPZkO8me",
        "outputId": "3e9d277a-f79b-4609-bfb8-c8a6f0ed6b3e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## creating a function to perform Question Answer analysis"
      ],
      "metadata": {
        "id": "vUk_PHiUPH0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(sample_question, sample_context):\n",
        "    input_ids = tokenizer.encode(sample_question, sample_context, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_logits)\n",
        "    end_index = torch.argmax(end_logits)\n",
        "\n",
        "    start_token = input_ids[0][start_index].item()\n",
        "    end_token = input_ids[0][end_index].item()\n",
        "\n",
        "    answer = tokenizer.decode(input_ids[0][start_index:end_index+1])\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "C-PB1JniNSUZ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample_question = \"How many weeks did their single \\\"Independent Women Part I\\\" stay on top?\"\n",
        "sample_context = \"The remaining band members recorded \\\"Independent Women Part I\\\", which appeared on the soundtrack to the 2000 film, Charlie's Angels. It became their best-charting single, topping the U.S. Billboard Hot 100 chart for eleven consecutive weeks. In early 2001, while Destiny's Child was completing their third album, Beyoncé landed a major role in the MTV made-for-television film, Carmen: A Hip Hopera, starring alongside American actor Mekhi Phifer. Set in Philadelphia, the film is a modern interpretation of the 19th century opera Carmen by French composer Georges Bizet. When the third album Survivor was released in May 2001, Luckett and Roberson filed a lawsuit claiming that the songs were aimed at them. The album debuted at number one on the U.S. Billboard 200, with first-week sales of 663,000 copies sold. The album spawned other number-one hits, \\\"Bootylicious\\\" and the title track, \\\"Survivor\\\", the latter of which earned the group a Grammy Award for Best R&B Performance by a Duo or Group with Vocals. After releasing their holiday album 8 Days of Christmas in October 2001, the group announced a hiatus to further pursue solo careers.\"\n",
        "\n",
        "answer = sample(sample_question, sample_context)\n",
        "print(\"Sample Question:\", sample_question)\n",
        "print(\"Sample Context:\", sample_context)\n",
        "print(\"Answer:\", answer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi6bBDi9NTBk",
        "outputId": "c2c89e7f-9270-4e04-904e-ed82bf10ae47"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Question: How many weeks did their single \"Independent Women Part I\" stay on top?\n",
            "Sample Context: The remaining band members recorded \"Independent Women Part I\", which appeared on the soundtrack to the 2000 film, Charlie's Angels. It became their best-charting single, topping the U.S. Billboard Hot 100 chart for eleven consecutive weeks. In early 2001, while Destiny's Child was completing their third album, Beyoncé landed a major role in the MTV made-for-television film, Carmen: A Hip Hopera, starring alongside American actor Mekhi Phifer. Set in Philadelphia, the film is a modern interpretation of the 19th century opera Carmen by French composer Georges Bizet. When the third album Survivor was released in May 2001, Luckett and Roberson filed a lawsuit claiming that the songs were aimed at them. The album debuted at number one on the U.S. Billboard 200, with first-week sales of 663,000 copies sold. The album spawned other number-one hits, \"Bootylicious\" and the title track, \"Survivor\", the latter of which earned the group a Grammy Award for Best R&B Performance by a Duo or Group with Vocals. After releasing their holiday album 8 Days of Christmas in October 2001, the group announced a hiatus to further pursue solo careers.\n",
            "Answer: eleven\n"
          ]
        }
      ]
    }
  ]
}