{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQk9G7nkN1EU"
      },
      "source": [
        "# Finetuning Question Answering on Bigbird\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_YTYbFn0FRL"
      },
      "source": [
        "**Fine_tuning Bigbird Extracitve Question Answering in PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdmrH8Tan8wC",
        "outputId": "41cdc475-173e-4724-e7ea-a55746885387"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.memory.empty_cache() -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPoexZG8L9w8"
      },
      "source": [
        "Install transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD2QijQuMC2l"
      },
      "source": [
        "!pip install -q transformers datasets"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEWH4YZrTluo",
        "outputId": "7a8ce01c-86e8-4f7c-dd95-42bbc4b1c7ef"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phCxmwEPMQlS"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbluAigMPqq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from transformers import BigBirdForQuestionAnswering, BigBirdTokenizerFast\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHIawrVPfG0"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeEbZWRVPl2B"
      },
      "source": [
        "from datasets import load_dataset"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beUalV5_Pnqm"
      },
      "source": [
        "### Load and split dataset, using small datasets for the sake of model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwoGxKpGJVpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed8e7fe-aad9-4e79-a452-221469bec798"
      },
      "source": [
        "train_data, valid_data = load_dataset('squad_v2', split='train[:1%]'), load_dataset('squad_v2', split='validation[:3%]')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n",
            "WARNING:datasets.builder:Found cached dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRvHqejN0Z4I",
        "outputId": "397d9e07-90e2-4b89-9779-29fc1d91b9f5"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56be85543aeaaa14008c9063',\n",
              " 'title': 'Beyoncé',\n",
              " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'question': 'When did Beyonce start becoming popular?',\n",
              " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSiNoRZDkdv0"
      },
      "source": [
        "### Getting correct answer text alignment and tokenizing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Pmox87IuwU"
      },
      "source": [
        "# Dataset cleaning and tokenization\n",
        "# BertTokenizerFast because python tokenizer do not have char_to_token functionality\n",
        "\n",
        "def correct_alignment(context, answer):\n",
        "\n",
        "    \"\"\" Description: This functions corrects the alignment of answers in the squad dataset that are sometimes off by one or 2 values also adds end_postion index.\n",
        "    \n",
        "    inputs: list of contexts and answers\n",
        "    outputs: Updated list that contains answer_end positions \"\"\"\n",
        "    \n",
        "    start_text = answer['text'][0]\n",
        "    start_idx = answer['answer_start'][0]\n",
        "    end_idx = start_idx + len(start_text)\n",
        "\n",
        "    # When alignment is okay\n",
        "    if context[start_idx:end_idx] == start_text:\n",
        "      return start_idx, end_idx    \n",
        "      # When alignment is off by 1 character\n",
        "    elif context[start_idx-1:end_idx-1] == start_text:\n",
        "      return start_idx-1, end_idx-1  \n",
        "      # when alignment is off by 2 characters\n",
        "    elif context[start_idx-2:end_idx-2] == start_text:\n",
        "      return start_idx-2, end_idx-2\n",
        "    else:\n",
        "      raise ValueError()"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhaGUEvdP_TA"
      },
      "source": [
        "### Tokenize our training dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigBirdForQuestionAnswering.from_pretrained(\"rubentito/bigbird-base-itc-mpdocvqa\")\n",
        "tokenizer = BigBirdTokenizerFast.from_pretrained(\"rubentito/bigbird-base-itc-mpdocvqa\")\n"
      ],
      "metadata": {
        "id": "XoThaBRk-QSg"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asking a Question"
      ],
      "metadata": {
        "id": "gXLGdyi9V8V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_num = np.random.randint(0,len(train_data))\n",
        "question = train_data[\"question\"][random_num]\n",
        "text = train_data[\"context\"][random_num]"
      ],
      "metadata": {
        "id": "8GbyHAjUIvi0"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let’s see how many tokens this question and text pair have."
      ],
      "metadata": {
        "id": "gwzqEVPHWE8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(question, text)\n",
        "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3jrAY1AIvpT",
        "outputId": "1a235bce-04c0-4330-f64e-6626426fb4bf"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input has a total of 318 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To look at what our tokenizer is doing, let’s just print out the tokens and their IDs."
      ],
      "metadata": {
        "id": "6X3AbK8_WMrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    print('{:8}{:8,}'.format(token,id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWGpklAlJ1N0",
        "outputId": "43f43f57-a456-41de-b573-66e91853c782"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]         65\n",
            "▁What      1,968\n",
            "▁term      3,482\n",
            "▁describes   8,578\n",
            "▁the         363\n",
            "▁qualities  14,583\n",
            "▁of          387\n",
            "▁the         363\n",
            "▁relationship   2,877\n",
            "▁between   1,123\n",
            "▁Fr        1,406\n",
            "é            266\n",
            "d            168\n",
            "é            266\n",
            "ric        1,274\n",
            "▁and         391\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "?            131\n",
            "[SEP]         66\n",
            "▁Although   5,001\n",
            "▁the         363\n",
            "▁two         835\n",
            "▁displayed   9,167\n",
            "▁great     1,150\n",
            "▁respect   2,562\n",
            "▁and         391\n",
            "▁admiration  28,607\n",
            "▁for         430\n",
            "▁each      1,224\n",
            "▁other       685\n",
            ",            112\n",
            "▁their       612\n",
            "▁friendship  14,839\n",
            "▁was         474\n",
            "▁uneasy   34,745\n",
            "▁and         391\n",
            "▁had         651\n",
            "▁some        718\n",
            "▁qualities  14,583\n",
            "▁of          387\n",
            "▁a           358\n",
            "▁love      1,943\n",
            "-            113\n",
            "hate      37,136\n",
            "▁relationship   2,877\n",
            ".            114\n",
            "▁Harold   24,088\n",
            "▁C           428\n",
            ".            114\n",
            "▁Sc        1,547\n",
            "hon       24,231\n",
            "berg       4,001\n",
            "▁believes   5,905\n",
            "▁that        427\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            "▁displayed   9,167\n",
            "▁a           358\n",
            "▁\"           467\n",
            "t            184\n",
            "inge      12,013\n",
            "▁of          387\n",
            "▁jealousy  35,495\n",
            "▁and         391\n",
            "▁spite    15,376\n",
            "\"            102\n",
            "▁towards   3,472\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "'s           439\n",
            "▁vir       5,810\n",
            "tu        28,148\n",
            "osity     16,680\n",
            "▁on          420\n",
            "▁the         363\n",
            "▁piano    19,233\n",
            ",            112\n",
            "▁and         391\n",
            "▁others    1,955\n",
            "▁have        524\n",
            "▁also        736\n",
            "▁argued    7,290\n",
            "▁that        427\n",
            "▁he          440\n",
            "▁had         651\n",
            "▁become    1,817\n",
            "▁enchanted  42,383\n",
            "▁with        452\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "'s           439\n",
            "▁theat    23,877\n",
            "ric        1,274\n",
            "ality      1,584\n",
            ",            112\n",
            "▁show      1,006\n",
            "manship   25,529\n",
            "▁and         391\n",
            "▁success   2,044\n",
            ".            114\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "▁was         474\n",
            "▁the         363\n",
            "▁d           389\n",
            "edi       13,841\n",
            "cat        9,347\n",
            "ee         1,554\n",
            "▁of          387\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            "'s           439\n",
            "▁Op        8,771\n",
            ".            114\n",
            "▁10          939\n",
            "▁            321\n",
            "É            234\n",
            "t            184\n",
            "udes       8,502\n",
            ",            112\n",
            "▁and         391\n",
            "▁his         566\n",
            "▁performance   2,955\n",
            "▁of          387\n",
            "▁them        707\n",
            "▁prompted  12,154\n",
            "▁the         363\n",
            "▁composer  26,878\n",
            "▁to          385\n",
            "▁write     3,652\n",
            "▁to          385\n",
            "▁H           468\n",
            "iller      4,766\n",
            ",            112\n",
            "▁\"           467\n",
            "I            141\n",
            "▁should      916\n",
            "▁like        689\n",
            "▁to          385\n",
            "▁rob       3,958\n",
            "▁him         784\n",
            "▁of          387\n",
            "▁the         363\n",
            "▁way         936\n",
            "▁he          440\n",
            "▁plays     5,442\n",
            "▁my          717\n",
            "▁studies   3,741\n",
            ".\"           627\n",
            "▁However   2,203\n",
            ",            112\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            "▁expressed   6,342\n",
            "▁annoyance  38,751\n",
            "▁in          388\n",
            "▁18        1,349\n",
            "43         3,660\n",
            "▁when        719\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "▁performed   6,258\n",
            "▁one         631\n",
            "▁of          387\n",
            "▁his         566\n",
            "▁n           400\n",
            "oc           521\n",
            "tur       36,691\n",
            "nes        2,617\n",
            "▁with        452\n",
            "▁the         363\n",
            "▁addition   3,191\n",
            "▁of          387\n",
            "▁numerous   6,510\n",
            "▁intricate  28,847\n",
            "▁em          896\n",
            "bel        6,768\n",
            "lish       1,937\n",
            "ments      1,003\n",
            ",            112\n",
            "▁at          480\n",
            "▁which       644\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            "▁remarked  25,099\n",
            "▁that        427\n",
            "▁he          440\n",
            "▁should      916\n",
            "▁play        812\n",
            "▁the         363\n",
            "▁music     2,748\n",
            "▁as          456\n",
            "▁written   3,295\n",
            "▁or          494\n",
            "▁not         508\n",
            "▁play        812\n",
            "▁it          441\n",
            "▁at          480\n",
            "▁all         578\n",
            ",            112\n",
            "▁forcing  10,934\n",
            "▁an          382\n",
            "▁apology  17,130\n",
            ".            114\n",
            "▁Most      4,143\n",
            "▁bi        3,283\n",
            "ographers  34,164\n",
            "▁of          387\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            "▁state     1,282\n",
            "▁that        427\n",
            "▁after       807\n",
            "▁this        529\n",
            "▁the         363\n",
            "▁two         835\n",
            "▁had         651\n",
            "▁little    1,411\n",
            "▁to          385\n",
            "▁do          567\n",
            "▁with        452\n",
            "▁each      1,224\n",
            "▁other       685\n",
            ",            112\n",
            "▁although   3,685\n",
            "▁in          388\n",
            "▁his         566\n",
            "▁letters   7,576\n",
            "▁dated    14,668\n",
            "▁as          456\n",
            "▁late      2,840\n",
            "▁as          456\n",
            "▁18        1,349\n",
            "48         2,881\n",
            "▁he          440\n",
            "▁still     1,092\n",
            "▁referred   6,513\n",
            "▁to          385\n",
            "▁him         784\n",
            "▁as          456\n",
            "▁\"           467\n",
            "my         1,921\n",
            "▁friend    1,646\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "\".         2,012\n",
            "▁Some      2,874\n",
            "▁commentators  25,800\n",
            "▁point     1,067\n",
            "▁to          385\n",
            "▁events    3,096\n",
            "▁in          388\n",
            "▁the         363\n",
            "▁two         835\n",
            "▁men       1,551\n",
            "'s           439\n",
            "▁romantic  14,449\n",
            "▁lives     3,261\n",
            "▁which       644\n",
            "▁led       3,058\n",
            "▁to          385\n",
            "▁a           358\n",
            "▁rift     36,889\n",
            "▁between   1,123\n",
            "▁them        707\n",
            ";            127\n",
            "▁there       713\n",
            "▁are         490\n",
            "▁claims    3,768\n",
            "▁that        427\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "▁had         651\n",
            "▁displayed   9,167\n",
            "▁jealousy  35,495\n",
            "▁of          387\n",
            "▁his         566\n",
            "▁mistress  37,870\n",
            "▁Marie    20,593\n",
            "▁d           389\n",
            "'            107\n",
            "Ag        10,363\n",
            "oult      26,056\n",
            "'s           439\n",
            "▁obsession  22,387\n",
            "▁with        452\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            ",            112\n",
            "▁while     1,082\n",
            "▁others    1,955\n",
            "▁believe   2,076\n",
            "▁that        427\n",
            "▁Cho      10,132\n",
            "pin       11,736\n",
            "▁had         651\n",
            "▁become    1,817\n",
            "▁concerned   5,314\n",
            "▁about       647\n",
            "▁Lis      28,647\n",
            "z            190\n",
            "t            184\n",
            "'s           439\n",
            "▁growing   4,058\n",
            "▁relationship   2,877\n",
            "▁with        452\n",
            "▁George    4,603\n",
            "▁Sand      3,938\n",
            ".            114\n",
            "[SEP]         66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first occurence of [SEP] token\n",
        "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "print(\"SEP token index: \", sep_idx)\n",
        "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
        "num_seg_a = sep_idx+1\n",
        "print(\"Number of tokens in segment A: \", num_seg_a)\n",
        "#number of tokens in segment B (text)\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "print(\"Number of tokens in segment B: \", num_seg_b)\n",
        "#creating the segment ids\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "#making sure that every input token has a segment id\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWpQjdgYIvt5",
        "outputId": "fd94128a-ecf3-4305-8b45-c6e70ee85fbc"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEP token index:  20\n",
            "Number of tokens in segment A:  21\n",
            "Number of tokens in segment B:  297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let’s now feed this to our model."
      ],
      "metadata": {
        "id": "nb7FSUYkWXs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
        "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84WVe8MmIvz5",
        "outputId": "9e47c3d8-2959-4b9e-a471-32c1958f4cfc"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 318 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens with highest start and end scores\n",
        "answer_start = torch.argmax(output.start_logits)\n",
        "answer_end = torch.argmax(output.end_logits)\n",
        "if answer_end >= answer_start:\n",
        "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
        "else:\n",
        "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
        "    \n",
        "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
        "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sChS31PLJlFN",
        "outputId": "8c175cd6-4017-4462-d7ba-c7b658a552fb"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question:\n",
            "What term describes the qualities of the relationship between frédéric and liszt?\n",
            "\n",
            "Answer:\n",
            "▁love - hate ▁relationship ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "JmlPOmwwN6GG"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_rXKPZkO8me",
        "outputId": "6d9ab1e6-17e2-419d-f690-6cac08452997"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigBirdForQuestionAnswering(\n",
              "  (bert): BigBirdModel(\n",
              "    (embeddings): BigBirdEmbeddings(\n",
              "      (word_embeddings): Embedding(50358, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(4096, 768)\n",
              "      (token_type_embeddings): Embedding(16, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BigBirdEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BigBirdLayer(\n",
              "          (attention): BigBirdAttention(\n",
              "            (self): BigBirdSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BigBirdSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BigBirdIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): NewGELUActivation()\n",
              "          )\n",
              "          (output): BigBirdOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_classifier): BigBirdForQuestionAnsweringHead(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (intermediate): BigBirdIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      (intermediate_act_fn): NewGELUActivation()\n",
              "    )\n",
              "    (output): BigBirdOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## creating a function to perform Question Answer analysis"
      ],
      "metadata": {
        "id": "vUk_PHiUPH0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(sample_question, sample_context):\n",
        "    input_ids = tokenizer.encode(sample_question, sample_context, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_logits)\n",
        "    end_index = torch.argmax(end_logits)\n",
        "\n",
        "    start_token = input_ids[0][start_index].item()\n",
        "    end_token = input_ids[0][end_index].item()\n",
        "\n",
        "    answer = tokenizer.decode(input_ids[0][start_index:end_index+1])\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "C-PB1JniNSUZ"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample_question = \"For what network, did Beyonce land a major movie role in?\"\n",
        "sample_context = \"The remaining band members recorded \\\"Independent Women Part I\\\", which appeared on the soundtrack to the 2000 film, Charlie's Angels. It became their best-charting single, topping the U.S. Billboard Hot 100 chart for eleven consecutive weeks. In early 2001, while Destiny's Child was completing their third album, Beyoncé landed a major role in the MTV made-for-television film, Carmen: A Hip Hopera, starring alongside American actor Mekhi Phifer. Set in Philadelphia, the film is a modern interpretation of the 19th century opera Carmen by French composer Georges Bizet. When the third album Survivor was released in May 2001, Luckett and Roberson filed a lawsuit claiming that the songs were aimed at them. The album debuted at number one on the U.S. Billboard 200, with first-week sales of 663,000 copies sold. The album spawned other number-one hits, \\\"Bootylicious\\ and the title track, \\\"Survivor\\\", the latter of which earned the group a Grammy Award for Best R&B Performance by a Duo or Group with Vocals. After releasing their holiday album 8 Days of Christmas in October 2001, the group announced a hiatus to further pursue solo careers.\"\n",
        "\n",
        "answer = sample(sample_question, sample_context)\n",
        "print(\"Sample Question:\", sample_question)\n",
        "print(\"Sample Context:\", sample_context)\n",
        "print(\"Answer:\", answer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi6bBDi9NTBk",
        "outputId": "6abdcc51-d212-4a81-b296-9bf0971c4178"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Question: For what network, did Beyonce land a major movie role in?\n",
            "Sample Context: The remaining band members recorded \"Independent Women Part I\", which appeared on the soundtrack to the 2000 film, Charlie's Angels. It became their best-charting single, topping the U.S. Billboard Hot 100 chart for eleven consecutive weeks. In early 2001, while Destiny's Child was completing their third album, Beyoncé landed a major role in the MTV made-for-television film, Carmen: A Hip Hopera, starring alongside American actor Mekhi Phifer. Set in Philadelphia, the film is a modern interpretation of the 19th century opera Carmen by French composer Georges Bizet. When the third album Survivor was released in May 2001, Luckett and Roberson filed a lawsuit claiming that the songs were aimed at them. The album debuted at number one on the U.S. Billboard 200, with first-week sales of 663,000 copies sold. The album spawned other number-one hits, \"Bootylicious\\ and the title track, \"Survivor\", the latter of which earned the group a Grammy Award for Best R&B Performance by a Duo or Group with Vocals. After releasing their holiday album 8 Days of Christmas in October 2001, the group announced a hiatus to further pursue solo careers.\n",
            "Answer: MTV made\n"
          ]
        }
      ]
    }
  ]
}